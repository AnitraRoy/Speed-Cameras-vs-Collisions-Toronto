# -*- coding: utf-8 -*-
"""K_Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ybdxBM6CeqIxaPWkIGAt74LdB51AlKfI
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
collisions = pd.read_csv("/content/drive/My Drive/MIE368 Project - Group 15/Code/collisions_dataset/collisions_dataset_new/collisions_enriched.csv")
speed_cameras = pd.read_csv("/content/drive/My Drive/MIE368 Project - Group 15/Code/speed_camera_dataset/speed_camera_clean_new/speed_cameras_clean.csv")

print("Collisions data:")
print(collisions.head())

print("\nSpeed cameras data:")
print(speed_cameras.head())

# drop rows with missing coordinates
collisions = collisions.dropna(subset=['lat', 'lon'])
speed_cameras = speed_cameras.dropna(subset=['lat', 'lon'])

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

coords = collisions[['lon', 'lat']].dropna()

# use elbow method to pick a best K
inertia = []
for k in range(2, 10):
    model = KMeans(n_clusters=k, n_init=10, random_state=42)
    model.fit(coords)
    inertia.append(model.inertia_)

plt.plot(range(2, 10), inertia, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

"""Based on the graph we can say that the optimal # of clusters is 4 as the biggest drop/elbow happens around 4. Afterwards, the line begins to flatten meaning that adding more clusters won't improve."""

import folium

kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)
collisions['Cluster'] = kmeans.fit_predict(coords)
colors = ['black', 'blue', 'red', 'green']

toronto_map = folium.Map(location=[43.7, -79.4], zoom_start=11)

# plot
for _, row in collisions.sample(2000, random_state=1).iterrows():
    folium.CircleMarker(
        location=[row['lat'], row['lon']],
        radius=2,
        color=colors[row['Cluster'] % len(colors)],
        fill=True,
        fill_opacity=0.6
    ).add_to(toronto_map)

# add blue markers for speed cameras
for _, row in speed_cameras.iterrows():
    folium.Marker(
        location=[row['lat'], row['lon']],
        icon=folium.Icon(color='blue', icon='camera')
    ).add_to(toronto_map)

toronto_map.save("toronto_collision_hotspots.html")

from google.colab import files
files.download("toronto_collision_hotspots.html")

toronto_map

colors = ['black', 'blue', 'red', 'green']

# count collisions per cluster
cluster_counts = collisions['Cluster'].value_counts().sort_index()

# calculate percentages
total = cluster_counts.sum()
percentages = (cluster_counts / total * 100).round(2)

print("Collisions per cluster:\n")
for i in range(len(cluster_counts)):
    print(f"Cluster {i} ({colors[i]}): {cluster_counts[i]} collisions ({percentages[i]}%)")

# map severity to numeric score
severity_map = {'Property Damage Only': 0, 'Injury': 1, 'Fatal': 2}
collisions['severity_score'] = collisions['severity'].map(severity_map)

# compute average severity per cluster
severity_by_cluster = (collisions.groupby('Cluster')['severity_score'].mean().round(3))

colors = ['black', 'blue', 'red', 'green']

for i, score in enumerate(severity_by_cluster):
    print(f"Cluster {i} ({colors[i]}): average severity = {score:.3f}")

import numpy as np
from sklearn.neighbors import BallTree

# compute collisions per camera ratio for each K-Means cluster zone
R = 6371000 # earth's radius

camera_coords = speed_cameras[['lon', 'lat']].to_numpy()
camera_labels = kmeans.predict(camera_coords)

camera_counts = np.bincount(camera_labels, minlength=kmeans.n_clusters)
collision_counts = collisions['Cluster'].value_counts().sort_index()

camera_counts_safe = np.where(camera_counts == 0, np.nan, camera_counts)

# compute ratio: collisions per camera = tot collisions in cluster / # of cameras in cluster
ratio = (collision_counts / camera_counts_safe).round(1)

print("Collisions per camera by cluster:\n")
for i in range(len(ratio)):
    if np.isnan(ratio[i]):
        print(f"Cluster {i} ({colors[i]}): No cameras in this cluster")
    else:
        print(f"Cluster {i} ({colors[i]}): {ratio[i]} collisions per camera")

hourly_summary = collisions.groupby('Cluster')['hour'].mean().round(1)
print(hourly_summary)

weather_summary = collisions.groupby('Cluster')[['wx_snow_on_ground', 'wx_avg_temperature']].mean().round(2)
print(weather_summary)

summary_df = pd.DataFrame({
    'Color': ['black', 'blue', 'red', 'green'],
    'Collisions': cluster_counts.values,
    'Percent of Total (%)': percentages.values,
    'Avg Severity Score': severity_by_cluster.values.round(3),
    'Collisions per Camera': ratio.values.round(1),
    'Avg Collision Hour': hourly_summary.values,
    'Avg Snow on Ground (cm)': weather_summary['wx_snow_on_ground'].values,
    'Avg Temperature (Â°C)': weather_summary['wx_avg_temperature'].values
})

summary_df.index.name = 'Cluster'
summary_df